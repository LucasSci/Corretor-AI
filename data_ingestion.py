"""
Sistema de ingest√£o de dados multi-fonte.
Permite adicionar conhecimento de PDFs, im√≥veis, websites, plantas, etc.
"""

import os
import json
import requests
from typing import List, Dict, Any, Optional
from pathlib import Path
from urllib.parse import urljoin
import PyPDF2
from bs4 import BeautifulSoup
import time

from knowledge_manager import intelligence_core


class DataIngestionPipeline:
    """Pipeline de ingest√£o de dados multi-fonte."""
    
    def __init__(self):
        self.intelligence_core = intelligence_core
    
    def _get_with_retries(self, url: str, timeout: int = 10, max_retries: int = 3) -> requests.Response:
        """Realiza um GET com retries para reduzir perda de p√°ginas lentas."""
        last_exc = None
        for attempt in range(1, max_retries + 1):
            try:
                resp = requests.get(url, timeout=timeout)
                return resp
            except (requests.Timeout, requests.ConnectionError) as e:
                last_exc = e
                print(f"    ‚ö†Ô∏è Tentativa {attempt}/{max_retries} falhou para {url}: {e}")
                time.sleep(1)
                continue
        raise last_exc
    
    # ============ PDFs ============
    def ingest_pdf(self, pdf_path: str, categoria: str = "documento", public: bool = True):
        """Ingere dados de um arquivo PDF.

        Args:
            public: se False, esse conte√∫do n√£o ser√° usado em respostas a clientes.
        """
        print(f"üìÑ Lendo PDF: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            print(f"‚ùå Arquivo n√£o encontrado: {pdf_path}")
            return
        
        documentos = []
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        documentos.append(text)
            
            if documentos:
                self.intelligence_core.add_training_data(
                    documents=documentos,
                    source=f"PDF: {os.path.basename(pdf_path)}",
                    category=categoria,
                    public=public
                )
                print(f"‚úÖ PDF processado: {len(documentos)} p√°gina(s)")
        
        except Exception as e:
            print(f"‚ùå Erro ao processar PDF: {e}")
    
    def ingest_pdf_folder(self, folder_path: str, categoria: str = "documento"):
        """Ingere todos os PDFs de uma pasta."""
        print(f"üìÅ Processando PDFs da pasta: {folder_path}")
        
        pdf_files = list(Path(folder_path).glob("**/*.pdf"))
        print(f"üìä {len(pdf_files)} PDF(s) encontrado(s)")
        
        for pdf_file in pdf_files:
            self.ingest_pdf(str(pdf_file), categoria)
    
    # ============ Im√≥veis ============
    def ingest_property(self, property_data: Dict[str, Any], public: bool = True):
        """Ingere informa√ß√µes de um im√≥vel.

        Args:
            public: se False, n√£o ser√° exposto a clientes
        """
        documentos = []
        
        # Montar texto estruturado do im√≥vel
        nome = property_data.get("nome", "Im√≥vel")
        descricao = property_data.get("descricao", "")
        localizacao = property_data.get("localizacao", "")
        amenidades = property_data.get("amenidades", [])
        precos = property_data.get("precos", {})
        
        # Documento 1: Descri√ß√£o geral
        doc_geral = f"""
IM√ìVEL: {nome}
LOCALIZA√á√ÉO: {localizacao}
DESCRI√á√ÉO: {descricao}
        """.strip()
        
        # Documento 2: Amenidades
        if amenidades:
            doc_amenidades = f"AMENIDADES do {nome}: {', '.join(amenidades)}"
            documentos.append(doc_amenidades)
        
        # Documento 3: Pre√ßos
        if precos:
            doc_precos = f"PRE√áOS {nome}: " + ", ".join([
                f"{tipo}: R$ {valor}" for tipo, valor in precos.items()
            ])
            documentos.append(doc_precos)
        
        documentos.insert(0, doc_geral)
        
        self.intelligence_core.add_training_data(
            documents=documentos,
            source=f"Im√≥vel: {nome}",
            category="imovel",
            public=public
        )
        
        print(f"‚úÖ Im√≥vel '{nome}' adicionado ao conhecimento")
    
    def ingest_properties_batch(self, properties_list: List[Dict[str, Any]]):
        """Ingere m√∫ltiplos im√≥veis."""
        print(f"üè¢ Adicionando {len(properties_list)} im√≥vel(eis)...")
        for prop in properties_list:
            self.ingest_property(prop)
    
    # ============ Websites ============
    def ingest_website(self, url: str, max_pages: int = 5, public: bool = True):
        """Ingere conte√∫do de um website.

        Args:
            public: se False, o conte√∫do n√£o ser√° usado em respostas ao cliente
        """
        print(f"üåê Explorando website: {url}")
        
        visited = set()
        to_visit = [url]
        documentos = []
        
        while to_visit and len(visited) < max_pages:
            current_url = to_visit.pop(0)
            
            if current_url in visited:
                continue
            
            visited.add(current_url)
            
            try:
                response = self._get_with_retries(current_url, timeout=10, max_retries=3)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Remover scripts e styles
                for script in soup(['script', 'style']):
                    script.decompose()
                
                text = soup.get_text(separator='\n', strip=True)
                
                if text:
                    documentos.append(text[:2000])  # Limitar tamanho
                
                # Encontrar links
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('/'):
                        href = urljoin(url, href)
                    
                    # Apenas links do mesmo dom√≠nio
                    if href.startswith(url) and href not in visited:
                        to_visit.append(href)
                
                print(f"  ‚úì {current_url} (p√°gina {len(visited)})")
            
            except Exception as e:
                print(f"  ‚úó Erro ao acessar {current_url}: {e}")
        
        if documentos:
            self.intelligence_core.add_training_data(
                documents=documentos,
                source=f"Website: {url}",
                category="website",
                public=public
            )
    def ingest_text_file(self, file_path: str, categoria: str = "documento"):
        """Ingere arquivo de texto."""
        print(f"üìù Lendo arquivo: {file_path}")
        
        if not os.path.exists(file_path):
            print(f"‚ùå Arquivo n√£o encontrado: {file_path}")
            return
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Dividir em chunks se muito grande
            chunk_size = 1000
            chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
            
            if chunks:
                self.intelligence_core.add_training_data(
                    documents=chunks,
                    source=f"Arquivo: {os.path.basename(file_path)}",
                    category=categoria
                )
                print(f"‚úÖ Arquivo processado: {len(chunks)} se√ß√£o(√µes)")
        
        except Exception as e:
            print(f"‚ùå Erro ao processar arquivo: {e}")
    
    def ingest_text_folder(self, folder_path: str, categoria: str = "documento"):
        """Ingere todos os arquivos de texto de uma pasta."""
        print(f"üìÅ Processando arquivos da pasta: {folder_path}")
        
        text_files = list(Path(folder_path).glob("**/*.txt")) + \
                     list(Path(folder_path).glob("**/*.md"))
        
        print(f"üìä {len(text_files)} arquivo(s) encontrado(s)")
        
        for text_file in text_files:
            self.ingest_text_file(str(text_file), categoria)
    
    # ============ JSON/CSV ============
    def ingest_json_file(self, json_path: str, categoria: str = "dados"):
        """Ingere dados estruturados de JSON."""
        print(f"üìã Lendo JSON: {json_path}")
        
        if not os.path.exists(json_path):
            print(f"‚ùå Arquivo n√£o encontrado: {json_path}")
            return
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Converter para strings leg√≠veis
            documentos = []
            if isinstance(data, dict):
                for key, value in data.items():
                    documentos.append(f"{key}: {json.dumps(value, ensure_ascii=False)}")
            elif isinstance(data, list):
                for item in data:
                    documentos.append(json.dumps(item, ensure_ascii=False))
            
            if documentos:
                self.intelligence_core.add_training_data(
                    documents=documentos,
                    source=f"JSON: {os.path.basename(json_path)}",
                    category=categoria
                )
                print(f"‚úÖ JSON processado: {len(documentos)} item(ens)")
        
        except Exception as e:
            print(f"‚ùå Erro ao processar JSON: {e}")
    
    # ============ Conhecimento Direto ============
    def add_custom_knowledge(self, knowledge_text: str, categoria: str = "custom", public: bool = True):
        """Adiciona conhecimento customizado diretamente.

        Args:
            public: se False, n√£o ser√° exposto ao cliente
        """
        documentos = [knowledge_text]
        
        self.intelligence_core.add_training_data(
            documents=documentos,
            source="Input direto",
            category=categoria,
            public=public
        )
        
        print(f"‚úÖ Conhecimento adicionado: {categoria}")    
    def ingest_multiple_websites(self, urls: List[str], max_pages_per_site: int = 3, public: bool = True):
        """Ingere m√∫ltiplos websites de uma vez."""
        print(f"\nüåê Ingerindo {len(urls)} website(s)...")
        total_conteudo = 0
        
        for i, url in enumerate(urls, 1):
            print(f"  [{i}/{len(urls)}] {url[:60]}...", end=" ")
            try:
                self.ingest_website(url, max_pages=max_pages_per_site, public=public)
                print("‚úÖ")
                total_conteudo += 1
            except Exception as e:
                print(f"‚ùå {str(e)[:30]}")
        
        print(f"\n‚úÖ {total_conteudo}/{len(urls)} websites ingeridos com sucesso!")
    
    def ingest_website_with_depth(self, url: str, max_depth: int = 2, max_pages: int = 10, public: bool = True):
        """
        Ingere website com controle de profundidade.
        
        Args:
            url: URL inicial
            max_depth: Profundidade m√°xima para crawl (0=p√°gina inicial, 1=links diretos, etc)
            max_pages: Quantidade m√°xima de p√°ginas a raspar
        """
        print(f"\nüåê Explorando website com profundidade {max_depth}: {url}")
        
        visited = set()
        to_visit = [(url, 0)]  # (url, depth)
        documentos = []
        
        while to_visit and len(visited) < max_pages:
            current_url, depth = to_visit.pop(0)
            
            if current_url in visited or depth > max_depth:
                continue
            
            visited.add(current_url)
            
            try:
                print(f"  {'  ' * depth}‚Ü≥ [{depth}] {current_url[:70]}...", end="")
                response = self._get_with_retries(current_url, timeout=10, max_retries=3)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Remover scripts e estilos
                for script in soup(['script', 'style']):
                    script.decompose()
                
                text = soup.get_text(separator='\n', strip=True)
                
                if text:
                    documentos.append(text[:5000])  # Limitar tamanho por p√°gina
                    print(f" ‚úÖ ({len(text)} chars)")
                else:
                    print(" (vazio)")
                
                # Encontrar links para pr√≥xima itera√ß√£o
                if depth < max_depth:
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if href.startswith('/'):
                            href = urljoin(url, href)
                        
                        # Apenas links do mesmo dom√≠nio
                        if href.startswith(url.rstrip('/')) and href not in visited:
                            if len(to_visit) < max_pages * 2:  # Evita crescimento infinito
                                to_visit.append((href, depth + 1))
            
            except Exception as e:
                print(f" ‚ùå {str(e)[:40]}")
        
        if documentos:
            print(f"\nüìù Adicionando {len(documentos)} p√°gina(s) √† base de conhecimento...")
            self.intelligence_core.add_training_data(
                documents=documentos,
                source=f"Website: {url}",
                category="website"
            )
            print(f"‚úÖ Website processado com sucesso! ({len(visited)} p√°ginas)")
        else:
            print(f"‚ö†Ô∏è Nenhum conte√∫do capturado")

# Inst√¢ncia global
ingestion_pipeline = DataIngestionPipeline()


if __name__ == "__main__":
    print("üîÑ Sistema de Ingest√£o de Dados Inicializado\n")
    
    # Exemplo: Ingerir im√≥veis
    exemplos_imoveis = [
        {
            "nome": "Apogeu Barra",
            "localizacao": "Barra da Tijuca, Rio de Janeiro",
            "descricao": "Empreendimento de luxo com acabamento premium",
            "amenidades": ["2 Piscinas", "Spa", "Academia", "Playground", "Sal√£o de Festas"],
            "precos": {
                "Studio": "450000",
                "1 Quarto": "650000",
                "2 Quartos": "950000"
            }
        },
        {
            "nome": "Duet Barra",
            "localizacao": "Barra da Tijuca, Rio de Janeiro",
            "descricao": "Apartamentos modernos com varanda gourmet",
            "amenidades": ["Piscina", "Quadra de Esportes", "Coworking"],
            "precos": {
                "2 Quartos": "800000",
                "3 Quartos": "1200000"
            }
        }
    ]
    
    ingestion_pipeline.ingest_properties_batch(exemplos_imoveis)
    
    # Exemplo: Adicionar conhecimento direto
    ingestion_pipeline.add_custom_knowledge(
        "Os im√≥veis de luxo em Barra da Tijuca s√£o ideais para clientes que buscam conforto, "
        "seguran√ßa e acesso a amenidades de primeira linha. A regi√£o oferece proximidade com "
        "shopping centers, restaurantes e praias.",
        categoria="dicas_venda"
    )
